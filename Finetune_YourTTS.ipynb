{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune YourTTS model for my voice dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this notebook I use the small dataset of my voice and finetune the YourTTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use pip install -e .[all] tts==0.7.1 to get the correct version of TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchaudio ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import Vits, VitsArgs\n",
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.utils.managers import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://coqui.gateway.scarf.sh/v0.6.1_models/tts_models--multilingual--multi-dataset--your_tts.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a directory YourTTS_models and save the contents of the above zip file in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'TTS_train/'\n",
    "REF_PATH = './YourTTS_models/'\n",
    "\n",
    "# create output path\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "# model vars \n",
    "MODEL_PATH = './YourTTS_models/model_file.pth'\n",
    "CONFIG_PATH = './YourTTS_models/config.json'\n",
    "TTS_LANGUAGES = \"./YourTTS_models/language_ids.json\"\n",
    "TTS_SPEAKERS = \"./YourTTS_models/speakers.json\"\n",
    "# SE = Speaker Encoder\n",
    "SE_MODEL_PATH=\"./YourTTS_models/model_se.pth\"\n",
    "CONFIG_SE_PATH = \"./YourTTS_models/config_se.json\"\n",
    "print(CONFIG_PATH)\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Embeddings from new custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify config_se.json to match the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is:\n",
    "```\n",
    "    /MyTTSDataset\n",
    "    | /en  \n",
    "        | -> metadata.txt\n",
    "        | -> /wavs\n",
    "            | -> audio1.wav\n",
    "            | -> audio2.wav\n",
    "            | ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.datasets import load_tts_samples\n",
    "# custom formatter implementation\n",
    "def customFormatter(root_path, manifest_file, **kwargs):  # pylint: disable=unused-argument\n",
    "    \"\"\"Assumes each line as ```<filename>|<transcription>```\n",
    "    \"\"\"\n",
    "    txt_file = os.path.join(root_path, manifest_file)\n",
    "    items = []\n",
    "    speaker_name = \"shiva\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.split(\"|\")\n",
    "            wav_file = os.path.join(root_path, \"wavs\", cols[0])+\".wav\"\n",
    "            text = cols[1]\n",
    "            items.append({'text':text, 'audio_file':wav_file, 'speaker_name':speaker_name, \\\n",
    "                            'audio_unique_name':cols[0]})\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code below is taken from compute_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"TTS_train\"\n",
    "dataset_config = BaseDatasetConfig()\n",
    "dataset_config.formatter = customFormatter\n",
    "dataset_config.dataset_name = \"shiva\"\n",
    "dataset_config.path = os.path.join(output_path, \"Shiva-1.0/en\")\n",
    "dataset_config.meta_file_train = \"metadata.txt\"\n",
    "dataset_config.language = \"en\"\n",
    "dataset_config.speaker_name = \"shiva\"\n",
    "# load training samples\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, \n",
    "                                               eval_split=True, \n",
    "                                               eval_split_size=0.1,\n",
    "                                               formatter=customFormatter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_samples + eval_samples\n",
    "print(samples)\n",
    "encoder_manager = SpeakerManager(\n",
    "    encoder_model_path= SE_MODEL_PATH,\n",
    "    encoder_config_path= CONFIG_SE_PATH,\n",
    "    d_vectors_file_path= None,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(encoder_manager.encoder_config.class_name_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_name_key = encoder_manager.encoder_config.class_name_key\n",
    "print(class_name_key)\n",
    "\n",
    "# compute speaker embeddings\n",
    "new_list = []\n",
    "speaker_mapping = {}\n",
    "embedding_key = \"shiva_en.wav\"\n",
    "for idx, fields in enumerate(tqdm(samples)):\n",
    "    #class_name = fields[class_name_key]\n",
    "    audio_file = fields[\"audio_file\"]\n",
    "    new_list.append(audio_file)\n",
    "    \n",
    "    \n",
    "# extract the embedding from the full list of audio files\n",
    "embedd = encoder_manager.compute_embedding_from_clip(new_list)\n",
    "\n",
    "# create speaker_mapping if target dataset is defined\n",
    "speaker_mapping[embedding_key] = {}\n",
    "speaker_mapping[embedding_key][\"name\"] = dataset_config.dataset_name + \"-en-1\\n\"\n",
    "speaker_mapping[embedding_key][\"embedding\"] = embedd\n",
    "\n",
    "if speaker_mapping:\n",
    "    # save speaker_mapping if target dataset is defined\n",
    "    if os.path.isdir(REF_PATH):\n",
    "        mapping_file_path = os.path.join(REF_PATH, \"speakers_mod.json\")\n",
    "\n",
    "\n",
    "    save_file(speaker_mapping, mapping_file_path)\n",
    "    print(\"Speaker embeddings saved at:\", mapping_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!!! Point the d_vector in config.json to speakers_mod.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config, config path was defined above\n",
    "C = load_config(CONFIG_PATH)\n",
    "\n",
    "\n",
    "# load the audio processor\n",
    "ap = AudioProcessor(**C.audio)\n",
    "\n",
    "speaker_embedding = None\n",
    "\n",
    "C.datasets = dataset_config\n",
    "C.model_args['d_vector_file'] = os.path.join(REF_PATH, \"speakers_mod.json\")\n",
    "C.model_args['use_speaker_encoder_as_loss'] = False\n",
    "C.model_args['speaker_encoder_config_path'] = CONFIG_SE_PATH\n",
    "C.model_args['speaker_encoder_model_path'] = SE_MODEL_PATH\n",
    "\n",
    "C.save_json(os.path.join(REF_PATH, \"config_mod.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train: Get set go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.eval() does not work\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = setup_model(C)\n",
    "# model.language_manager.set_language_ids_from_file(TTS_LANGUAGES)\n",
    "# print(model.language_manager.num_languages, model.embedded_language_dim)\n",
    "# print(model.emb_l)\n",
    "cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "# remove speaker encoder\n",
    "model_weights = cp['model'].copy()\n",
    "for key in list(model_weights.keys()):\n",
    "  if \"speaker_encoder\" in key:\n",
    "    del model_weights[key]\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# set languages\n",
    "model.language_manager.set_language_ids_from_file(TTS_LANGUAGES)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# synthesize voice\n",
    "use_griffin_lim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This system command does not go through\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG_PATH = os.path.join(REF_PATH + \"config_mod.json\")\n",
    "command = f\"python ./TTS/TTS/bin/train_tts.py --config_path {NEW_CONFIG_PATH} --restore_path {MODEL_PATH}\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "!tensorboard --logdir=TTS_train/<run_dir>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "output_path = \"TTS_train\"\n",
    "ckpt = output_path + \"best_model.pth\"\n",
    "config = output_path + \"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tts --text \"Is it recognize speech or wreck a nice beach?\" \\\n",
    "      --model_path $ckpt \\\n",
    "      --config_path $config \\\n",
    "      --out_path test1.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listen to the synthesized speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"test1.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38tts': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5058910fdc281ff0cea44e05d7ce1c0555f0d90c4a43084b3046c094e190c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
