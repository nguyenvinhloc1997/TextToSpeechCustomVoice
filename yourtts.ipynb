{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is used to do custom voice TTS using [Coqui](https://github.com/coqui-ai/TTS) YourTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and install Coqui as per yourtts_readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete steps 1 and 2 of the readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchaudio ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tts --list_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the YourTTS model and configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://coqui.gateway.scarf.sh/v0.6.1_models/tts_models--multilingual--multi-dataset--your_tts.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow Step 3 in the readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "TTS_PATH = \"TTS/\"\n",
    "\n",
    "# add libraries into environment\n",
    "sys.path.append(TTS_PATH) # set this if TTS is not installed globally\n",
    "\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "\n",
    "# This line was picked from https://colab.research.google.com/drive/1r0NDBxxW5RZjQ1Jy99XohnY6thYWNBCd?usp=sharing#scrollTo=2akFqoi7UiD4\n",
    "# This import was not resolved\n",
    "# from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "try:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "except:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.models.vits import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths for model and config downloaded (yourtts_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'yourtts_out/'\n",
    "\n",
    "# create output path\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "# model vars \n",
    "MODEL_PATH = './yourtts_models/model_file.pth'\n",
    "CONFIG_PATH = './yourtts_models/config.json'\n",
    "\n",
    "TTS_SPEAKERS = \"./yourtts_models/speakers.json\"\n",
    "# SE = Speaker Encoder\n",
    "SE_MODEL_PATH=\"./yourtts_models/model_se.pth\"\n",
    "CONFIG_SE_PATH = \"./yourtts_models/config_se.json\"\n",
    "print(CONFIG_PATH)\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config, config path was defined above\n",
    "C = load_config(CONFIG_PATH)\n",
    "\n",
    "\n",
    "# load the audio processor\n",
    "ap = AudioProcessor(**C.audio)\n",
    "\n",
    "speaker_embedding = None\n",
    "\n",
    "C.model_args['d_vector_file'] = TTS_SPEAKERS\n",
    "C.model_args['use_speaker_encoder_as_loss'] = False\n",
    "C.model_args['speaker_encoder_config_path'] = CONFIG_SE_PATH\n",
    "C.model_args['speaker_encoder_model_path'] = SE_MODEL_PATH\n",
    "\n",
    "model = setup_model(C)\n",
    "# model.language_manager.set_language_ids_from_file(TTS_LANGUAGES)\n",
    "# print(model.language_manager.num_languages, model.embedded_language_dim)\n",
    "# print(model.emb_l)\n",
    "cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important - remove the speaker encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove speaker encoder\n",
    "model_weights = cp['model'].copy()\n",
    "for key in list(model_weights.keys()):\n",
    "  if \"speaker_encoder\" in key:\n",
    "    del model_weights[key]\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# synthesize voice\n",
    "use_griffin_lim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker Encoder Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q pydub ffmpeg-normalize==1.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from pydub import AudioSegment\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the Speaker Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_speaker_manager = SpeakerManager(encoder_model_path=SE_MODEL_PATH, \\\n",
    "                                    encoder_config_path=CONFIG_SE_PATH, \\\n",
    "                                        use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spec(ref_file):\n",
    "  y, sr = librosa.load(ref_file, sr=ap.sample_rate)\n",
    "  spec = ap.spectrogram(y)\n",
    "  spec = torch.FloatTensor(spec).unsqueeze(0)\n",
    "  return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voice Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload, normalize and resample your wav files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload wav files - I have saved the wavs in Libri-speech format - directory MyTTSDataset/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAVS_PATH = \"./MyTTSDataset/wavs/\"\n",
    "reference_files = os.listdir(WAVS_PATH)\n",
    "new_list = [ WAVS_PATH+s for s in reference_files]\n",
    "\n",
    "#print(new_list)\n",
    "for sample in new_list:\n",
    "    !ffmpeg-normalize $sample -nt rms -t=-27 -o $sample -ar 16000 -f\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_emb = SE_speaker_manager.compute_embedding_from_clip(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define inference variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.length_scale = 1  # scaler for the duration predictor. The larger it is, the slower the speech.\n",
    "model.inference_noise_scale = 0.3 # defines the noise variance applied to the random z vector at inference.\n",
    "model.inference_noise_scale_dp = 0.3 # defines the noise variance applied to the duration predictor z vector at inference.\n",
    "text = \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Choose language id\n",
    "model.language_manager.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_id = 0 # english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "print(\" > text: {}\".format(text))\n",
    "wav, alignment, _, _ = synthesis(\n",
    "                    model,\n",
    "                    text,\n",
    "                    C,\n",
    "                    use_cuda = USE_CUDA,\n",
    "                    speaker_id=None,\n",
    "                    style_wav=None,\n",
    "                    use_griffin_lim=True,\n",
    "                    do_trim_silence=False,\n",
    "                    d_vector=reference_emb,\n",
    "                    language_id=language_id,\n",
    "                ).values()\n",
    "print(\"Generated Audio\")\n",
    "IPython.display.display(Audio(wav, rate=ap.sample_rate))\n",
    "file_name = \"text1.wav\"\n",
    "#file_name = file_name.translate(str.maketrans('', '', string.punctuation.replace('_', ''))) + '.wav'\n",
    "out_path = os.path.join(OUT_PATH, file_name)\n",
    "print(\" > Saving output to {}\".format(out_path))\n",
    "ap.save_wav(wav, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This did not work - stores an empty file\n",
    "# \n",
    "# emb_file = \"emb_file.json\"\n",
    "# print(SE_speaker_manager.get_speakers())\n",
    "# SE_speaker_manager.save_embeddings_to_file(os.path.join(OUT_PATH, emb_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('newTTSEnv0.7.1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63752043fe54a03a91bc972c9672c2d8a9d97333a84a3a175c41a6b0a9e6c3bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
